# ==============================================
# PLUGSPACE.IO TITAN v1.4 - PRODUCTION DOCKER COMPOSE
# ==============================================
# Enterprise-grade container orchestration with:
# - MongoDB 7 replica set for high availability
# - Redis 7 with persistence and clustering
# - Nginx reverse proxy with SSL/TLS termination
# - Health check containers for monitoring
# - Automated backup containers
# - Log aggregation with centralized logging
# - Resource limits and restart policies
# - Network isolation between services
# ==============================================

version: '3.9'

x-common-env: &common-env
  NODE_ENV: ${NODE_ENV:-production}
  TZ: ${TZ:-UTC}

x-healthcheck-defaults: &healthcheck-defaults
  interval: 30s
  timeout: 10s
  retries: 3
  start_period: 40s

x-logging: &logging
  driver: json-file
  options:
    max-size: "50m"
    max-file: "10"
    labels: "service,env"

x-resource-limits-api: &resource-limits-api
  deploy:
    resources:
      limits:
        cpus: '2.0'
        memory: 2G
      reservations:
        cpus: '0.5'
        memory: 512M

x-resource-limits-worker: &resource-limits-worker
  deploy:
    resources:
      limits:
        cpus: '1.0'
        memory: 1G
      reservations:
        cpus: '0.25'
        memory: 256M

services:
  # ============================================
  # DATABASE SERVICES
  # ============================================

  mongo-primary:
    image: mongo:7
    container_name: plugspace-mongo-primary
    hostname: mongo-primary
    command: ["mongod", "--replSet", "rs0", "--bind_ip_all", "--keyFile", "/data/keyfile/mongodb-keyfile"]
    environment:
      <<: *common-env
      MONGO_INITDB_ROOT_USERNAME: ${MONGO_ROOT_USERNAME:-admin}
      MONGO_INITDB_ROOT_PASSWORD: ${MONGO_ROOT_PASSWORD}
      MONGO_INITDB_DATABASE: plugspace
    ports:
      - "27017:27017"
    volumes:
      - mongo-primary-data:/data/db
      - mongo-primary-config:/data/configdb
      - ./infrastructure/mongodb/keyfile:/data/keyfile:ro
      - ./infrastructure/mongodb/init:/docker-entrypoint-initdb.d:ro
    networks:
      - plugspace-database
    restart: unless-stopped
    healthcheck:
      <<: *healthcheck-defaults
      test: ["CMD", "mongosh", "--eval", "db.adminCommand('ping')"]
    logging: *logging
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '0.5'
          memory: 1G

  mongo-secondary1:
    image: mongo:7
    container_name: plugspace-mongo-secondary1
    hostname: mongo-secondary1
    command: ["mongod", "--replSet", "rs0", "--bind_ip_all", "--keyFile", "/data/keyfile/mongodb-keyfile"]
    environment:
      <<: *common-env
    volumes:
      - mongo-secondary1-data:/data/db
      - mongo-secondary1-config:/data/configdb
      - ./infrastructure/mongodb/keyfile:/data/keyfile:ro
    networks:
      - plugspace-database
    restart: unless-stopped
    depends_on:
      mongo-primary:
        condition: service_healthy
    healthcheck:
      <<: *healthcheck-defaults
      test: ["CMD", "mongosh", "--eval", "db.adminCommand('ping')"]
    logging: *logging
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
        reservations:
          cpus: '0.25'
          memory: 512M

  mongo-secondary2:
    image: mongo:7
    container_name: plugspace-mongo-secondary2
    hostname: mongo-secondary2
    command: ["mongod", "--replSet", "rs0", "--bind_ip_all", "--keyFile", "/data/keyfile/mongodb-keyfile"]
    environment:
      <<: *common-env
    volumes:
      - mongo-secondary2-data:/data/db
      - mongo-secondary2-config:/data/configdb
      - ./infrastructure/mongodb/keyfile:/data/keyfile:ro
    networks:
      - plugspace-database
    restart: unless-stopped
    depends_on:
      mongo-primary:
        condition: service_healthy
    healthcheck:
      <<: *healthcheck-defaults
      test: ["CMD", "mongosh", "--eval", "db.adminCommand('ping')"]
    logging: *logging
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
        reservations:
          cpus: '0.25'
          memory: 512M

  mongo-init-replica:
    image: mongo:7
    container_name: plugspace-mongo-init
    depends_on:
      mongo-primary:
        condition: service_healthy
      mongo-secondary1:
        condition: service_healthy
      mongo-secondary2:
        condition: service_healthy
    environment:
      MONGO_ROOT_USERNAME: ${MONGO_ROOT_USERNAME:-admin}
      MONGO_ROOT_PASSWORD: ${MONGO_ROOT_PASSWORD}
    volumes:
      - ./infrastructure/mongodb/init-replica.sh:/init-replica.sh:ro
    networks:
      - plugspace-database
    entrypoint: ["/bin/bash", "/init-replica.sh"]
    restart: "no"

  # ============================================
  # CACHE & QUEUE SERVICES
  # ============================================

  redis-master:
    image: redis:7-alpine
    container_name: plugspace-redis-master
    hostname: redis-master
    command: >
      redis-server
      --requirepass ${REDIS_PASSWORD}
      --maxmemory 512mb
      --maxmemory-policy allkeys-lru
      --appendonly yes
      --appendfsync everysec
      --save 900 1
      --save 300 10
      --save 60 10000
      --tcp-keepalive 300
      --timeout 0
      --loglevel notice
    ports:
      - "6379:6379"
    volumes:
      - redis-master-data:/data
    networks:
      - plugspace-cache
    restart: unless-stopped
    healthcheck:
      <<: *healthcheck-defaults
      test: ["CMD", "redis-cli", "-a", "${REDIS_PASSWORD}", "ping"]
    logging: *logging
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 256M

  redis-replica:
    image: redis:7-alpine
    container_name: plugspace-redis-replica
    hostname: redis-replica
    command: >
      redis-server
      --slaveof redis-master 6379
      --masterauth ${REDIS_PASSWORD}
      --requirepass ${REDIS_PASSWORD}
      --maxmemory 512mb
      --maxmemory-policy allkeys-lru
      --appendonly yes
      --tcp-keepalive 300
    volumes:
      - redis-replica-data:/data
    networks:
      - plugspace-cache
    depends_on:
      redis-master:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      <<: *healthcheck-defaults
      test: ["CMD", "redis-cli", "-a", "${REDIS_PASSWORD}", "ping"]
    logging: *logging
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 128M

  redis-sentinel:
    image: redis:7-alpine
    container_name: plugspace-redis-sentinel
    command: >
      redis-sentinel /etc/redis/sentinel.conf
    volumes:
      - ./infrastructure/redis/sentinel.conf:/etc/redis/sentinel.conf:ro
    networks:
      - plugspace-cache
    depends_on:
      - redis-master
      - redis-replica
    restart: unless-stopped
    logging: *logging

  # ============================================
  # APPLICATION SERVICES
  # ============================================

  api:
    build:
      context: .
      dockerfile: ./apps/api/Dockerfile
      args:
        NODE_ENV: ${NODE_ENV:-production}
    container_name: plugspace-api
    environment:
      <<: *common-env
      PORT: 4000
      DATABASE_URL: mongodb://${MONGO_ROOT_USERNAME}:${MONGO_ROOT_PASSWORD}@mongo-primary:27017,mongo-secondary1:27017,mongo-secondary2:27017/plugspace?replicaSet=rs0&authSource=admin
      REDIS_URL: redis://:${REDIS_PASSWORD}@redis-master:6379/0
      FIREBASE_PROJECT_ID: ${FIREBASE_PROJECT_ID}
      FIREBASE_CLIENT_EMAIL: ${FIREBASE_CLIENT_EMAIL}
      FIREBASE_PRIVATE_KEY: ${FIREBASE_PRIVATE_KEY}
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY}
      GOOGLE_AI_API_KEY: ${GOOGLE_AI_API_KEY}
      JWT_SECRET: ${JWT_SECRET}
      ENCRYPTION_KEY: ${ENCRYPTION_KEY}
      LOG_LEVEL: ${LOG_LEVEL:-info}
      SENTRY_DSN: ${SENTRY_DSN}
    ports:
      - "4000:4000"
    networks:
      - plugspace-backend
      - plugspace-database
      - plugspace-cache
    depends_on:
      mongo-primary:
        condition: service_healthy
      redis-master:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      <<: *healthcheck-defaults
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:4000/health"]
    logging: *logging
    <<: *resource-limits-api

  landing:
    build:
      context: .
      dockerfile: ./apps/landing/Dockerfile
      args:
        NODE_ENV: ${NODE_ENV:-production}
    container_name: plugspace-landing
    environment:
      <<: *common-env
      PORT: 3000
      NEXT_PUBLIC_API_URL: ${API_URL:-http://api:4000}
    ports:
      - "3000:3000"
    networks:
      - plugspace-frontend
      - plugspace-backend
    depends_on:
      api:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      <<: *healthcheck-defaults
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:3000/api/health"]
    logging: *logging
    <<: *resource-limits-worker

  studio:
    build:
      context: .
      dockerfile: ./apps/studio/Dockerfile
      args:
        NODE_ENV: ${NODE_ENV:-production}
    container_name: plugspace-studio
    environment:
      <<: *common-env
      PORT: 3001
      NEXT_PUBLIC_API_URL: ${API_URL:-http://api:4000}
    ports:
      - "3001:3001"
    networks:
      - plugspace-frontend
      - plugspace-backend
    depends_on:
      api:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      <<: *healthcheck-defaults
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:3001/api/health"]
    logging: *logging
    <<: *resource-limits-worker

  admin:
    build:
      context: .
      dockerfile: ./apps/admin/Dockerfile
      args:
        NODE_ENV: ${NODE_ENV:-production}
    container_name: plugspace-admin
    environment:
      <<: *common-env
      PORT: 3002
      NEXT_PUBLIC_API_URL: ${API_URL:-http://api:4000}
      ADMIN_EMAIL: ${ADMIN_EMAIL:-plugspaceapp@gmail.com}
    ports:
      - "3002:3002"
    networks:
      - plugspace-frontend
      - plugspace-backend
    depends_on:
      api:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      <<: *healthcheck-defaults
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:3002/api/health"]
    logging: *logging
    <<: *resource-limits-worker

  # ============================================
  # REVERSE PROXY & LOAD BALANCING
  # ============================================

  nginx:
    image: nginx:1.25-alpine
    container_name: plugspace-nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./infrastructure/nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./infrastructure/nginx/conf.d:/etc/nginx/conf.d:ro
      - ./infrastructure/nginx/ssl:/etc/nginx/ssl:ro
      - nginx-cache:/var/cache/nginx
      - nginx-logs:/var/log/nginx
      - ./infrastructure/certbot/conf:/etc/letsencrypt:ro
      - ./infrastructure/certbot/www:/var/www/certbot:ro
    networks:
      - plugspace-frontend
      - plugspace-backend
    depends_on:
      - landing
      - studio
      - admin
      - api
    restart: unless-stopped
    healthcheck:
      <<: *healthcheck-defaults
      test: ["CMD", "nginx", "-t"]
    logging: *logging
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 64M

  certbot:
    image: certbot/certbot:latest
    container_name: plugspace-certbot
    volumes:
      - ./infrastructure/certbot/conf:/etc/letsencrypt
      - ./infrastructure/certbot/www:/var/www/certbot
      - certbot-logs:/var/log/letsencrypt
    entrypoint: "/bin/sh -c 'trap exit TERM; while :; do certbot renew; sleep 12h & wait $${!}; done;'"
    networks:
      - plugspace-frontend
    restart: unless-stopped
    logging: *logging

  # ============================================
  # MONITORING & OBSERVABILITY
  # ============================================

  prometheus:
    image: prom/prometheus:v2.48.0
    container_name: plugspace-prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
    ports:
      - "9090:9090"
    volumes:
      - ./infrastructure/prometheus:/etc/prometheus:ro
      - prometheus-data:/prometheus
    networks:
      - plugspace-monitoring
      - plugspace-backend
    restart: unless-stopped
    healthcheck:
      <<: *healthcheck-defaults
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:9090/-/healthy"]
    logging: *logging
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 256M

  grafana:
    image: grafana/grafana:10.2.2
    container_name: plugspace-grafana
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_ADMIN_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD}
      GF_INSTALL_PLUGINS: grafana-clock-panel,grafana-simple-json-datasource
      GF_SERVER_ROOT_URL: ${GRAFANA_ROOT_URL:-http://localhost:3003}
      GF_SMTP_ENABLED: ${GF_SMTP_ENABLED:-false}
      GF_SMTP_HOST: ${GF_SMTP_HOST:-}
      GF_SMTP_USER: ${GF_SMTP_USER:-}
      GF_SMTP_PASSWORD: ${GF_SMTP_PASSWORD:-}
    ports:
      - "3003:3000"
    volumes:
      - grafana-data:/var/lib/grafana
      - ./infrastructure/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./infrastructure/grafana/datasources:/etc/grafana/provisioning/datasources:ro
    networks:
      - plugspace-monitoring
    depends_on:
      - prometheus
    restart: unless-stopped
    healthcheck:
      <<: *healthcheck-defaults
      test: ["CMD-SHELL", "wget -q --spider http://localhost:3000/api/health || exit 1"]
    logging: *logging
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 128M

  alertmanager:
    image: prom/alertmanager:v0.26.0
    container_name: plugspace-alertmanager
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
    ports:
      - "9093:9093"
    volumes:
      - ./infrastructure/alertmanager:/etc/alertmanager:ro
      - alertmanager-data:/alertmanager
    networks:
      - plugspace-monitoring
    restart: unless-stopped
    logging: *logging

  node-exporter:
    image: prom/node-exporter:v1.7.0
    container_name: plugspace-node-exporter
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    ports:
      - "9100:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    networks:
      - plugspace-monitoring
    restart: unless-stopped
    logging: *logging
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 128M

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.47.2
    container_name: plugspace-cadvisor
    privileged: true
    ports:
      - "8080:8080"
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    networks:
      - plugspace-monitoring
    restart: unless-stopped
    logging: *logging
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 256M

  # ============================================
  # LOG AGGREGATION
  # ============================================

  loki:
    image: grafana/loki:2.9.2
    container_name: plugspace-loki
    ports:
      - "3100:3100"
    command: -config.file=/etc/loki/local-config.yaml
    volumes:
      - ./infrastructure/loki:/etc/loki:ro
      - loki-data:/loki
    networks:
      - plugspace-monitoring
    restart: unless-stopped
    logging: *logging
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M

  promtail:
    image: grafana/promtail:2.9.2
    container_name: plugspace-promtail
    volumes:
      - ./infrastructure/promtail:/etc/promtail:ro
      - /var/log:/var/log:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - nginx-logs:/var/log/nginx:ro
    command: -config.file=/etc/promtail/config.yml
    networks:
      - plugspace-monitoring
    depends_on:
      - loki
    restart: unless-stopped
    logging: *logging

  # ============================================
  # BACKUP SERVICES
  # ============================================

  mongo-backup:
    image: mongo:7
    container_name: plugspace-mongo-backup
    environment:
      MONGO_HOST: mongo-primary
      MONGO_PORT: 27017
      MONGO_DB: plugspace
      MONGO_USER: ${MONGO_ROOT_USERNAME:-admin}
      MONGO_PASS: ${MONGO_ROOT_PASSWORD}
      BACKUP_SCHEDULE: ${BACKUP_SCHEDULE:-0 2 * * *}
      BACKUP_RETENTION_DAYS: ${BACKUP_RETENTION_DAYS:-30}
      S3_BUCKET: ${S3_BACKUP_BUCKET:-}
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID:-}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY:-}
    volumes:
      - ./infrastructure/backup/mongo-backup.sh:/backup.sh:ro
      - mongo-backups:/backups
    networks:
      - plugspace-database
    depends_on:
      mongo-primary:
        condition: service_healthy
    entrypoint: ["/bin/bash", "-c", "crond -f -d 8"]
    restart: unless-stopped
    logging: *logging

  redis-backup:
    image: redis:7-alpine
    container_name: plugspace-redis-backup
    environment:
      REDIS_HOST: redis-master
      REDIS_PORT: 6379
      REDIS_PASSWORD: ${REDIS_PASSWORD}
      BACKUP_SCHEDULE: ${BACKUP_SCHEDULE:-0 3 * * *}
    volumes:
      - ./infrastructure/backup/redis-backup.sh:/backup.sh:ro
      - redis-backups:/backups
    networks:
      - plugspace-cache
    depends_on:
      redis-master:
        condition: service_healthy
    entrypoint: ["/bin/sh", "-c", "crond -f -d 8"]
    restart: unless-stopped
    logging: *logging

  # ============================================
  # HEALTH CHECK AGGREGATOR
  # ============================================

  healthcheck:
    image: curlimages/curl:8.5.0
    container_name: plugspace-healthcheck
    volumes:
      - ./infrastructure/healthcheck/check.sh:/check.sh:ro
    networks:
      - plugspace-backend
      - plugspace-database
      - plugspace-cache
    depends_on:
      - api
      - mongo-primary
      - redis-master
    entrypoint: ["/bin/sh", "-c", "while true; do sh /check.sh; sleep 60; done"]
    restart: unless-stopped
    logging: *logging

# ============================================
# NETWORKS
# ============================================

networks:
  plugspace-frontend:
    driver: bridge
    name: plugspace-frontend
    ipam:
      config:
        - subnet: 172.20.0.0/24

  plugspace-backend:
    driver: bridge
    name: plugspace-backend
    ipam:
      config:
        - subnet: 172.21.0.0/24

  plugspace-database:
    driver: bridge
    name: plugspace-database
    internal: true
    ipam:
      config:
        - subnet: 172.22.0.0/24

  plugspace-cache:
    driver: bridge
    name: plugspace-cache
    internal: true
    ipam:
      config:
        - subnet: 172.23.0.0/24

  plugspace-monitoring:
    driver: bridge
    name: plugspace-monitoring
    ipam:
      config:
        - subnet: 172.24.0.0/24

# ============================================
# VOLUMES
# ============================================

volumes:
  # MongoDB volumes
  mongo-primary-data:
    driver: local
    name: plugspace-mongo-primary-data
  mongo-primary-config:
    driver: local
    name: plugspace-mongo-primary-config
  mongo-secondary1-data:
    driver: local
    name: plugspace-mongo-secondary1-data
  mongo-secondary1-config:
    driver: local
    name: plugspace-mongo-secondary1-config
  mongo-secondary2-data:
    driver: local
    name: plugspace-mongo-secondary2-data
  mongo-secondary2-config:
    driver: local
    name: plugspace-mongo-secondary2-config
  mongo-backups:
    driver: local
    name: plugspace-mongo-backups

  # Redis volumes
  redis-master-data:
    driver: local
    name: plugspace-redis-master-data
  redis-replica-data:
    driver: local
    name: plugspace-redis-replica-data
  redis-backups:
    driver: local
    name: plugspace-redis-backups

  # Nginx volumes
  nginx-cache:
    driver: local
    name: plugspace-nginx-cache
  nginx-logs:
    driver: local
    name: plugspace-nginx-logs

  # Monitoring volumes
  prometheus-data:
    driver: local
    name: plugspace-prometheus-data
  grafana-data:
    driver: local
    name: plugspace-grafana-data
  alertmanager-data:
    driver: local
    name: plugspace-alertmanager-data
  loki-data:
    driver: local
    name: plugspace-loki-data
  certbot-logs:
    driver: local
    name: plugspace-certbot-logs
